
services:
  # ðŸ§  Database: PostgreSQL + pgvector
  pgvector:
    image: agnohq/pgvector:16
    container_name: pgvector
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${DB_USER:-ai}
      POSTGRES_PASSWORD: ${DB_PASS:-change-me-in-production}
      POSTGRES_DB: ${DB_DATABASE:-ai}
      POSTGRES_INITDB_ARGS: "-c shared_preload_libraries=pgvector"
    ports:
      - "${DB_PORT:-5432}:5432"
    volumes:
      - pgvolume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-ai}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          cpus: "1"
          memory: 1G
    networks:
      - vboarder-network
    labels:
      service: "database"
      component: "pgvector"

  # ðŸ¤– Ollama (GPU-accelerated LLM engine)
  ollama:
    image: ollama/ollama:latest
    container_name: vboarder_ollama
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      OLLAMA_KEEP_ALIVE: "${OLLAMA_KEEP_ALIVE:-4h}"
      OLLAMA_NUM_PARALLEL: "${OLLAMA_NUM_PARALLEL:-2}"
      OLLAMA_NUM_CTX: "${OLLAMA_NUM_CTX:-4096}"
      OLLAMA_NUM_THREADS: "${OLLAMA_NUM_THREADS:-4}"
    volumes:
      - ollama_models:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: ["gpu"]
        limits:
          cpus: "4"
          memory: 8G
    networks:
      - vboarder-network
    labels:
      service: "inference"
      component: "ollama"

  # ðŸš€ VBoarder-Agno API (FastAPI backend)
  vboarder-agno-api:
    build:
      context: ./app
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    container_name: vboarder-agno-api
    restart: unless-stopped
    depends_on:
      pgvector:
        condition: service_healthy
      ollama:
        condition: service_healthy
    env_file:
      - .env
    environment:
      DB_HOST: pgvector
      OLLAMA_HOST: http://ollama:11434
    ports:
      - "${API_PORT:-8000}:8000"
    command: >
      uvicorn app_optimized:app
      --host 0.0.0.0
      --port 8000
      --reload
      --log-level info
    volumes:
      - ./app:/app
      - /app/__pycache__
      - /app/.pytest_cache
    healthcheck:
      # âœ… FIX: Include API key header in health check
      test: ["CMD-SHELL", "curl -fsS -H 'X-API-Key: ${AGENT_API_KEY:-supersecret-local-key}' http://localhost:8000/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          cpus: "1"
          memory: 1G
    networks:
      - vboarder-network
    labels:
      service: "api"
      component: "fastapi"

  # ðŸ’¬ AgentUI (Next.js frontend)
  agent-ui:
    build:
      context: ./agent-ui
      dockerfile: Dockerfile
      args:
        - BUILDKIT_INLINE_CACHE=1
    container_name: agent-ui
    restart: unless-stopped
    depends_on:
      vboarder-agno-api:
        condition: service_healthy
    environment:
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      NODE_ENV: ${NODE_ENV:-development}
    ports:
      - "${UI_PORT:-3000}:3000"
    volumes:
      - ./agent-ui:/usr/src/app
      - /usr/src/app/.next
      - /usr/src/app/node_modules
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 512M
    networks:
      - vboarder-network
    labels:
      service: "frontend"
      component: "nextjs"

networks:
  vboarder-network:
    driver: bridge

volumes:
  pgvolume:
    driver: local
  ollama_models:
    driver: local
